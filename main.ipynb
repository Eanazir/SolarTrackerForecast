{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "def preprocess_meteorological_data(file_path):\n",
    "    # Load data\n",
    "    met_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Combine DATE and MST into a single datetime column\n",
    "    # met_data['Timestamp'] = pd.to_datetime(met_data['DATE'] + ' ' + met_data['MST'])\n",
    "    # Combine DATE and MST into a single datetime column\n",
    "    met_data['Timestamp'] = pd.to_datetime(met_data['datetime'].astype(str), format='%Y%m%d%H%M%S')\n",
    "\n",
    "    # Sort data by Timestamp\n",
    "    met_data.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "    # Handle missing values in input features using KNN imputation\n",
    "    input_features = [\n",
    "        'Tower Dry Bulb Temp [deg C]', 'Tower RH [%]', 'Station Pressure [mBar]',\n",
    "        'Avg Wind Speed @ 6ft [m/s]', 'Avg Wind Direction @ 6ft [deg from N]'\n",
    "    ]\n",
    "\n",
    "    # Initialize KNN imputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # Fit and transform the input features\n",
    "    met_data_imputed = imputer.fit_transform(met_data[input_features])\n",
    "\n",
    "    # Update the DataFrame with imputed values\n",
    "    met_data[input_features] = met_data_imputed\n",
    "\n",
    "    # Handle missing values in the target variable separately\n",
    "    target_variable = 'Global CMP22 (vent/cor) [W/m^2]'\n",
    "\n",
    "    # Optionally interpolate missing target values\n",
    "    # met_data[target_variable].interpolate(method='time', inplace=True)\n",
    "    \n",
    "    # Option 2: Drop rows with missing target values (uncomment if preferred)\n",
    "    met_data.dropna(subset=[target_variable], inplace=True)\n",
    "\n",
    "    # Rename columns for simplicity\n",
    "    met_data.rename(columns={\n",
    "        'Tower Dry Bulb Temp [deg C]': 'Temperature',\n",
    "        'Tower RH [%]': 'Humidity',\n",
    "        'Station Pressure [mBar]': 'Pressure',\n",
    "        'Avg Wind Speed @ 6ft [m/s]': 'Wind Speed',\n",
    "        'Avg Wind Direction @ 6ft [deg from N]': 'Wind Direction',\n",
    "        'Global CMP22 (vent/cor) [W/m^2]': 'Irradiance'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Feature scaling for input features\n",
    "    scaler = MinMaxScaler()\n",
    "    met_data[['Temperature', 'Humidity', 'Pressure', 'Wind Speed']] = scaler.fit_transform(\n",
    "        met_data[['Temperature', 'Humidity', 'Pressure', 'Wind Speed']]\n",
    "    )\n",
    "    joblib.dump(scaler, 'scaler_y.pkl')\n",
    "\n",
    "    # Wind Direction encoding (convert degrees to sine and cosine components)\n",
    "    met_data['Wind Dir Sin'] = np.sin(np.deg2rad(met_data['Wind Direction']))\n",
    "    met_data['Wind Dir Cos'] = np.cos(np.deg2rad(met_data['Wind Direction']))\n",
    "    met_data.drop('Wind Direction', axis=1, inplace=True)\n",
    "\n",
    "    # Temporal features\n",
    "    met_data['Hour'] = met_data['Timestamp'].dt.hour / 23.0  # Normalize Hour\n",
    "    met_data['DayOfYear'] = met_data['Timestamp'].dt.dayofyear / 365.0  # Normalize DayOfYear\n",
    "\n",
    "    # Prepare target variables (future irradiance)\n",
    "    target = 'Irradiance'\n",
    "    for minutes in [5, 15, 30, 60]:\n",
    "        met_data[f'Irradiance_{minutes}min_ahead'] = met_data[target].shift(-minutes)\n",
    "\n",
    "    # **Removed the line that drops rows with NaN values after shifting**\n",
    "    # We will handle dropping NaN values after merging with images\n",
    "    # met_data.dropna(inplace=True)\n",
    "\n",
    "    # Reset index and return the processed DataFrame\n",
    "    return met_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/image_preprocessing.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_images(image_folder):\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_folder, '*.jpg')))\n",
    "    images = []\n",
    "    image_timestamps = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        # Extract timestamp from image filename\n",
    "        # Assuming filename format: YYYYMMDDHHMMSS.jpg\n",
    "        filename = os.path.basename(path)\n",
    "        timestamp_str = filename.replace('.jpg', '')\n",
    "        timestamp = pd.to_datetime(timestamp_str, format='%Y%m%d%H%M%S')\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue  # Skip if the image is not readable\n",
    "        # Iterates over each image path, reads the image using OpenCV, and resizes it to 128x128 pixels\n",
    "\t    # Normalizes pixel values to the range [0, 1] by dividing by 255\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        img = img / 255.0  # Normalize pixel values\n",
    "        images.append(img)\n",
    "        image_timestamps.append(timestamp)\n",
    "\n",
    "    return images, image_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_data = preprocess_meteorological_data('weather_data.csv')\n",
    "met_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, image_timestamps = preprocess_images('pics')\n",
    "len(image_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 499 records.\n",
      "Testing data contains 125 records.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Align Data with Images\n",
    "def align_data_with_images(met_data, images, image_timestamps):\n",
    "    \"\"\"\n",
    "    Aligns meteorological data with corresponding images based on timestamps.\n",
    "\n",
    "    Parameters:\n",
    "    - met_data: pandas DataFrame containing meteorological data.\n",
    "    - images: list or numpy array of preprocessed images.\n",
    "    - image_timestamps: list or pandas Series of image timestamps.\n",
    "\n",
    "    Returns:\n",
    "    - merged_data: pandas DataFrame containing aligned meteorological data and images.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for image timestamps\n",
    "    image_df = pd.DataFrame({'Timestamp': image_timestamps, 'Image': images})\n",
    "    image_df['Timestamp'] = pd.to_datetime(image_df['Timestamp'])\n",
    "    \n",
    "    # Ensure meteorological data has datetime objects\n",
    "    met_data['Timestamp'] = pd.to_datetime(met_data['Timestamp'])\n",
    "    \n",
    "    # Merge meteorological data with images using an inner join to keep only matching timestamps\n",
    "    merged_data = pd.merge(met_data, image_df, on='Timestamp', how='inner')\n",
    "    \n",
    "    # Prepare target variables (future irradiance)\n",
    "    target = 'Irradiance'\n",
    "    for minutes in [5, 15, 30, 60]:\n",
    "        merged_data[f'Irradiance_{minutes}min_ahead'] = merged_data[target].shift(-minutes)\n",
    "    \n",
    "    # Drop rows with any remaining missing values (due to shifting)\n",
    "    merged_data.dropna(inplace=True)\n",
    "    \n",
    "    # Reset index after dropping rows\n",
    "    merged_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "merged_data = align_data_with_images(met_data, images, image_timestamps)\n",
    "\n",
    "# Step 3: Split Data into Training and Testing Sets BEFORE Sequence Creation\n",
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "split_index = int(len(merged_data) * split_ratio)\n",
    "\n",
    "# Perform the split\n",
    "train_data = merged_data.iloc[:split_index]\n",
    "test_data = merged_data.iloc[split_index:]\n",
    "\n",
    "print(f\"Training data contains {len(train_data)} records.\")\n",
    "print(f\"Testing data contains {len(test_data)} records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/train_model.py\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from models.hybrid import create_hybrid_model  # Ensure correct import path\n",
    "import pandas as pd\n",
    "\n",
    "def train_hybrid_model(train_data, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Trains the hybrid CNN-LSTM model on the provided training data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: pandas DataFrame containing training meteorological data and images.\n",
    "    - sequence_length: Number of past minutes to consider for each sequence.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained Keras model.\n",
    "    - history: Training history object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define feature columns\n",
    "    features = [\n",
    "        'Temperature', 'Humidity', 'Pressure', 'Wind Speed',\n",
    "        'Wind Dir Sin', 'Wind Dir Cos', 'Hour', 'DayOfYear'\n",
    "    ]  # Exclude direct current irradiance as a feature\n",
    "\n",
    "    # Extract features and targets\n",
    "    X_num_train, X_img_train, y_train = create_sequences(train_data, sequence_length, features)\n",
    "\n",
    "    print(f\"Created {len(X_num_train)} training sequences.\")\n",
    "\n",
    "    # Create and compile the model\n",
    "    num_features = X_num_train.shape[2]\n",
    "    model = create_hybrid_model(sequence_length, num_features)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "\n",
    "    # Train the model with dictionary inputs\n",
    "    history = model.fit(\n",
    "        {'Image_Input': X_img_train, 'LSTM_Input': X_num_train},\n",
    "        y_train,\n",
    "        validation_split=0.1,  # Further split training data for validation\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def create_sequences(data, seq_length, features):\n",
    "    \"\"\"\n",
    "    Creates input sequences and corresponding targets from the data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the data.\n",
    "    - seq_length: Length of each input sequence.\n",
    "    - features: List of feature column names.\n",
    "\n",
    "    Returns:\n",
    "    - X_num_seq: Numpy array of numerical feature sequences.\n",
    "    - X_img_seq: Numpy array of image sequences.\n",
    "    - y_seq: Numpy array of target irradiance values.\n",
    "    \"\"\"\n",
    "    X_num_seq, X_img_seq, y_seq = [], [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        # Extract numerical features for the sequence\n",
    "        X_num_seq.append(data[features].values[i:i+seq_length])\n",
    "        \n",
    "        # Extract the corresponding image at the last timestamp of the sequence\n",
    "        X_img_seq.append(data['Image'].values[i+seq_length-1])\n",
    "        \n",
    "        # Extract the target irradiance values at the end of the sequence\n",
    "        y_seq.append(data[[f'Irradiance_{minutes}min_ahead' for minutes in [5, 15, 30, 60]]].values[i+seq_length-1])\n",
    "    \n",
    "    return np.array(X_num_seq), np.array(X_img_seq), np.array(y_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 439 training sequences.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['Image_Input', 'LSTM_Input']. Received: the structure of inputs={'Image_Input': '*', 'LSTM_Input': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - loss: 177137.6250 - mae: 391.5333 - val_loss: 143341.7969 - val_mae: 355.1158 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 132ms/step - loss: 48604.2656 - mae: 177.2811 - val_loss: 100337.8203 - val_mae: 305.9944 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step - loss: 25964.6992 - mae: 140.5359 - val_loss: 90704.2500 - val_mae: 299.3154 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 142ms/step - loss: 19079.4746 - mae: 121.5322 - val_loss: 96893.2031 - val_mae: 310.8622 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - loss: 15244.1357 - mae: 108.4160 - val_loss: 52859.3750 - val_mae: 207.5781 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - loss: 29951.2227 - mae: 136.7463 - val_loss: 67544.4688 - val_mae: 254.7000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - loss: 14259.8613 - mae: 98.3082 - val_loss: 40477.9531 - val_mae: 197.3661 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - loss: 11600.2812 - mae: 91.1657 - val_loss: 29155.2891 - val_mae: 163.7463 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 131ms/step - loss: 8471.5371 - mae: 72.5848 - val_loss: 18420.6484 - val_mae: 125.5458 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - loss: 7163.7827 - mae: 67.8837 - val_loss: 17608.6270 - val_mae: 119.6790 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 150ms/step - loss: 6630.0576 - mae: 64.3742 - val_loss: 16391.3418 - val_mae: 113.6791 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - loss: 6147.0166 - mae: 61.9787 - val_loss: 17661.4043 - val_mae: 117.1712 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 131ms/step - loss: 5710.0835 - mae: 59.7858 - val_loss: 13199.1445 - val_mae: 99.1018 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - loss: 5655.6050 - mae: 59.8965 - val_loss: 10788.0400 - val_mae: 86.9775 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 144ms/step - loss: 6324.3105 - mae: 64.1879 - val_loss: 16166.6885 - val_mae: 108.0340 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - loss: 5742.5840 - mae: 59.6813 - val_loss: 14175.1846 - val_mae: 100.3942 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 143ms/step - loss: 5693.7446 - mae: 58.6644 - val_loss: 14716.1025 - val_mae: 101.5457 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 148ms/step - loss: 5366.1104 - mae: 58.2472 - val_loss: 11967.5508 - val_mae: 90.3298 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 161ms/step - loss: 5280.6348 - mae: 57.4175 - val_loss: 11543.1846 - val_mae: 87.9014 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the Model\n",
    "model, history = train_hybrid_model(train_data, sequence_length=60)\n",
    "\n",
    "# Save the model\n",
    "model.save('trainedModels/test.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation/evaluate_model.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def evaluate_model(model, test_data, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Evaluates the regression model on the provided test_data.\n",
    "    Saves evaluation plots in a uniquely named subfolder within 'evaluation_plots'.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained Keras model.\n",
    "    - test_data: pandas DataFrame containing testing meteorological data and images.\n",
    "    - sequence_length: Number of past minutes to consider for each sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 1. Setup Directory for Saving Plots\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Define the base directory for evaluation plots\n",
    "    base_dir = 'evaluation_plots'\n",
    "    \n",
    "    # Create the base directory if it doesn't exist\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        print(f\"Created base directory for evaluation plots at '{base_dir}'.\")\n",
    "    \n",
    "    # Generate a unique subfolder name using the current timestamp\n",
    "    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_folder = os.path.join(base_dir, f'run_{run_timestamp}')\n",
    "    \n",
    "    # Create the subfolder\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    print(f\"Created run-specific directory at '{run_folder}'.\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 2. Prepare Data for Evaluation\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Define feature columns\n",
    "    features = [\n",
    "        'Temperature', 'Humidity', 'Pressure', 'Wind Speed',\n",
    "        'Wind Dir Sin', 'Wind Dir Cos', 'Hour', 'DayOfYear'\n",
    "    ]  # Exclude direct current irradiance as a feature\n",
    "\n",
    "    # Extract features and targets\n",
    "    X_num = test_data[features].values\n",
    "    y = test_data[[f'Irradiance_{minutes}min_ahead' for minutes in [5, 15, 30, 60]]].values\n",
    "    X_img = np.array(test_data['Image'].tolist())\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Create Sequences\n",
    "    # -----------------------------\n",
    "    \n",
    "    def create_sequences(X_num, X_img, y, seq_length):\n",
    "        \"\"\"\n",
    "        Creates input sequences for the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - X_num: Numpy array of numerical features.\n",
    "        - X_img: Numpy array of images.\n",
    "        - y: Numpy array of target variables.\n",
    "        - seq_length: Length of the input sequences.\n",
    "        \n",
    "        Returns:\n",
    "        - Tuple of Numpy arrays: (X_num_seq, X_img_seq, y_seq)\n",
    "        \"\"\"\n",
    "        X_num_seq, X_img_seq, y_seq = [], [], []\n",
    "        for i in range(len(X_num) - seq_length):\n",
    "            X_num_seq.append(X_num[i:i+seq_length])\n",
    "            X_img_seq.append(X_img[i+seq_length-1])  # Use image at the last timestamp\n",
    "            y_seq.append(y[i+seq_length-1])\n",
    "        return np.array(X_num_seq), np.array(X_img_seq), np.array(y_seq)\n",
    "\n",
    "    # Generate sequences from test data\n",
    "    X_num_seq, X_img_seq, y_seq = create_sequences(X_num, X_img, y, sequence_length)\n",
    "    print(f\"Created {len(X_num_seq)} sequences for evaluation.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. No Further Splitting Needed\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Since test_data is already separate, no need to split again\n",
    "    X_num_test = X_num_seq\n",
    "    X_img_test = X_img_seq\n",
    "    y_test = y_seq\n",
    "    \n",
    "    print(f\"Evaluation split: {len(X_num_test)} samples.\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 5. Make Predictions\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Generate predictions using the trained model\n",
    "    y_pred = model.predict({'Image_Input': X_img_test, 'LSTM_Input': X_num_test})\n",
    "    print(\"Generated predictions for the test set.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6. Calculate and Save Metrics and Plots\n",
    "    # -----------------------------\n",
    "    \n",
    "    horizons = [5, 15, 30, 60]  # Prediction horizons in minutes\n",
    "    \n",
    "    # Initialize a text file to save metrics\n",
    "    metrics_file = os.path.join(run_folder, 'metrics.txt')\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        f.write(\"Evaluation Metrics:\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "    \n",
    "    for i, minutes in enumerate(horizons):\n",
    "        # Calculate metrics for each horizon\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[:, i], y_pred[:, i]))\n",
    "        mae = mean_absolute_error(y_test[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_test[:, i], y_pred[:, i])\n",
    "        metric_str = f\"{minutes}-Minute Ahead Prediction - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.2f}\"\n",
    "        print(metric_str)\n",
    "        \n",
    "        # Append metrics to the text file\n",
    "        with open(metrics_file, 'a') as f:\n",
    "            f.write(metric_str + \"\\n\")\n",
    "        \n",
    "        # Plot Actual vs Predicted\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(y_test[:, i], label='Actual', alpha=0.7)\n",
    "        plt.plot(y_pred[:, i], label='Predicted', alpha=0.7)\n",
    "        plt.title(f'{minutes}-Minute Ahead Prediction')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Irradiance (W/m²)')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_filename = f'{minutes}_min_ahead_prediction.png'\n",
    "        plot_path = os.path.join(run_folder, plot_filename)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {plot_path}\")\n",
    "        \n",
    "        # Plot Scatter of Actual vs Predicted\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(y_test[:, i], y_pred[:, i], alpha=0.5)\n",
    "        plt.plot([y_test[:, i].min(), y_test[:, i].max()],\n",
    "                 [y_test[:, i].min(), y_test[:, i].max()],\n",
    "                 'r--', lw=2)\n",
    "        plt.title(f'Actual vs Predicted Irradiance ({minutes} min Ahead)')\n",
    "        plt.xlabel('Actual Irradiance (W/m²)')\n",
    "        plt.ylabel('Predicted Irradiance (W/m²)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the scatter plot\n",
    "        scatter_filename = f'actual_vs_predicted_{minutes}_min_ahead.png'\n",
    "        scatter_path = os.path.join(run_folder, scatter_filename)\n",
    "        plt.savefig(scatter_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved scatter plot: {scatter_path}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 7. Calculate and Save Overall Metrics and Plots\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Calculate overall performance metrics across all horizons\n",
    "    overall_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    overall_mae = mean_absolute_error(y_test, y_pred)\n",
    "    overall_r2 = r2_score(y_test, y_pred)\n",
    "    overall_metric_str = f\"Overall Performance - RMSE: {overall_rmse:.2f}, MAE: {overall_mae:.2f}, R²: {overall_r2:.2f}\"\n",
    "    print(overall_metric_str)\n",
    "    \n",
    "    # Append overall metrics to the text file\n",
    "    with open(metrics_file, 'a') as f:\n",
    "        f.write(\"\\n\" + overall_metric_str + \"\\n\")\n",
    "    \n",
    "    # Plot Overall Actual vs Predicted\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()],\n",
    "             [y_test.min(), y_test.max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title('Actual vs Predicted Irradiance (Overall)')\n",
    "    plt.xlabel('Actual Irradiance (W/m²)')\n",
    "    plt.ylabel('Predicted Irradiance (W/m²)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overall scatter plot\n",
    "    overall_scatter_filename = 'actual_vs_predicted_overall.png'\n",
    "    overall_scatter_path = os.path.join(run_folder, overall_scatter_filename)\n",
    "    plt.savefig(overall_scatter_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved overall scatter plot: {overall_scatter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created run-specific directory at 'evaluation_plots/run_20241119_211151'.\n",
      "Created 65 sequences for evaluation.\n",
      "Evaluation split: 65 samples.\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 204ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['Image_Input', 'LSTM_Input']. Received: the structure of inputs={'Image_Input': '*', 'LSTM_Input': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Generated predictions for the test set.\n",
      "5-Minute Ahead Prediction - RMSE: 143.93, MAE: 133.36, R²: -34.46\n",
      "Saved plot: evaluation_plots/run_20241119_211151/5_min_ahead_prediction.png\n",
      "Saved scatter plot: evaluation_plots/run_20241119_211151/actual_vs_predicted_5_min_ahead.png\n",
      "15-Minute Ahead Prediction - RMSE: 118.69, MAE: 107.22, R²: -10.87\n",
      "Saved plot: evaluation_plots/run_20241119_211151/15_min_ahead_prediction.png\n",
      "Saved scatter plot: evaluation_plots/run_20241119_211151/actual_vs_predicted_15_min_ahead.png\n",
      "30-Minute Ahead Prediction - RMSE: 108.67, MAE: 95.90, R²: -4.32\n",
      "Saved plot: evaluation_plots/run_20241119_211151/30_min_ahead_prediction.png\n",
      "Saved scatter plot: evaluation_plots/run_20241119_211151/actual_vs_predicted_30_min_ahead.png\n",
      "60-Minute Ahead Prediction - RMSE: 71.20, MAE: 51.93, R²: -0.63\n",
      "Saved plot: evaluation_plots/run_20241119_211151/60_min_ahead_prediction.png\n",
      "Saved scatter plot: evaluation_plots/run_20241119_211151/actual_vs_predicted_60_min_ahead.png\n",
      "Overall Performance - RMSE: 113.67, MAE: 97.10, R²: -12.57\n",
      "Saved overall scatter plot: evaluation_plots/run_20241119_211151/actual_vs_predicted_overall.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_data, sequence_length=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
