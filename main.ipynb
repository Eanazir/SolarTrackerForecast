{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "def preprocess_meteorological_data(file_path):\n",
    "    # Load data\n",
    "    met_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Combine DATE and MST into a single datetime column\n",
    "    # met_data['Timestamp'] = pd.to_datetime(met_data['DATE'] + ' ' + met_data['MST'])\n",
    "    # Combine DATE and MST into a single datetime column\n",
    "    met_data['Timestamp'] = pd.to_datetime(met_data['datetime'].astype(str), format='%Y%m%d%H%M%S')\n",
    "\n",
    "    # Sort data by Timestamp\n",
    "    met_data.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "    # Handle missing values in input features using KNN imputation\n",
    "    input_features = [\n",
    "        'Tower Dry Bulb Temp [deg C]', 'Tower RH [%]', 'Station Pressure [mBar]',\n",
    "        'Avg Wind Speed @ 6ft [m/s]', 'Avg Wind Direction @ 6ft [deg from N]'\n",
    "    ]\n",
    "\n",
    "    # Initialize KNN imputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # Fit and transform the input features\n",
    "    met_data_imputed = imputer.fit_transform(met_data[input_features])\n",
    "\n",
    "    # Update the DataFrame with imputed values\n",
    "    met_data[input_features] = met_data_imputed\n",
    "\n",
    "    # Handle missing values in the target variable separately\n",
    "    target_variable = 'Global CMP22 (vent/cor) [W/m^2]'\n",
    "\n",
    "    # Optionally interpolate missing target values\n",
    "    # met_data[target_variable].interpolate(method='time', inplace=True)\n",
    "    \n",
    "    # Option 2: Drop rows with missing target values (uncomment if preferred)\n",
    "    met_data.dropna(subset=[target_variable], inplace=True)\n",
    "\n",
    "    # Rename columns for simplicity\n",
    "    met_data.rename(columns={\n",
    "        'Tower Dry Bulb Temp [deg C]': 'Temperature',\n",
    "        'Tower RH [%]': 'Humidity',\n",
    "        'Station Pressure [mBar]': 'Pressure',\n",
    "        'Avg Wind Speed @ 6ft [m/s]': 'Wind Speed',\n",
    "        'Avg Wind Direction @ 6ft [deg from N]': 'Wind Direction',\n",
    "        'Global CMP22 (vent/cor) [W/m^2]': 'Irradiance'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Feature scaling for input features\n",
    "    scaler = MinMaxScaler()\n",
    "    met_data[['Temperature', 'Humidity', 'Pressure', 'Wind Speed']] = scaler.fit_transform(\n",
    "        met_data[['Temperature', 'Humidity', 'Pressure', 'Wind Speed']]\n",
    "    )\n",
    "    joblib.dump(scaler, 'scaler_y.pkl')\n",
    "\n",
    "    # Wind Direction encoding (convert degrees to sine and cosine components)\n",
    "    met_data['Wind Dir Sin'] = np.sin(np.deg2rad(met_data['Wind Direction']))\n",
    "    met_data['Wind Dir Cos'] = np.cos(np.deg2rad(met_data['Wind Direction']))\n",
    "    met_data.drop('Wind Direction', axis=1, inplace=True)\n",
    "\n",
    "    # Temporal features\n",
    "    met_data['Hour'] = met_data['Timestamp'].dt.hour / 23.0  # Normalize Hour\n",
    "    met_data['DayOfYear'] = met_data['Timestamp'].dt.dayofyear / 365.0  # Normalize DayOfYear\n",
    "\n",
    "    # Prepare target variables (future irradiance)\n",
    "    target = 'Irradiance'\n",
    "    for minutes in [5, 15, 30, 60]:\n",
    "        met_data[f'Irradiance_{minutes}min_ahead'] = met_data[target].shift(-minutes)\n",
    "\n",
    "    # **Removed the line that drops rows with NaN values after shifting**\n",
    "    # We will handle dropping NaN values after merging with images\n",
    "    # met_data.dropna(inplace=True)\n",
    "\n",
    "    # Reset index and return the processed DataFrame\n",
    "    return met_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/image_preprocessing.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def process_image(path):\n",
    "    \"\"\"Process a single image\"\"\"\n",
    "    try:\n",
    "        # Extract timestamp from filename\n",
    "        filename = os.path.basename(path)\n",
    "        timestamp_str = filename.replace('.jpg', '')\n",
    "        timestamp = pd.to_datetime(timestamp_str, format='%Y%m%d%H%M%S')\n",
    "        \n",
    "        # Read and process image\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        \n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        img = img / 255.0\n",
    "        \n",
    "        return (img, timestamp)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_images(image_folder, n_workers=None):\n",
    "    \"\"\"Parallel image preprocessing\"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = os.cpu_count()\n",
    "    \n",
    "    # Get sorted image paths\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_folder, '*.jpg')))\n",
    "    \n",
    "    # Process images in parallel\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(process_image, image_paths),\n",
    "            total=len(image_paths),\n",
    "            desc=\"Processing images\"\n",
    "        ))\n",
    "    \n",
    "    # Filter out None results and unzip the valid results\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    images, timestamps = zip(*valid_results) if valid_results else ([], [])\n",
    "    \n",
    "    return list(images), list(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_data = preprocess_meteorological_data('data/combined_data.csv')\n",
    "met_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, image_timestamps = preprocess_images('/data/training_images/')\n",
    "# image_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/train_model.py\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from models.hybrid import create_hybrid_model\n",
    "import pandas as pd\n",
    "\n",
    "def train_hybrid_model(met_data, images, image_timestamps):\n",
    "    # Prepare data\n",
    "    sequence_length = 60  # Number of past minutes to consider\n",
    "    features = ['Temperature', 'Humidity', 'Pressure', 'Wind Speed', 'Wind Dir Sin', 'Wind Dir Cos',\n",
    "                'Hour', 'DayOfYear']\n",
    "\n",
    "    # Align images with meteorological data\n",
    "    merged_data = align_data_with_images(met_data, images, image_timestamps)\n",
    "\n",
    "    # Extract features and targets\n",
    "    X_num = merged_data[features].values\n",
    "    y = merged_data[[f'Irradiance_{minutes}min_ahead' for minutes in [5, 15, 30, 60]]].values\n",
    "    X_img = np.array(merged_data['Image'].tolist())\n",
    "\n",
    "    # Create sequences\n",
    "    def create_sequences(X_num, X_img, y, seq_length):\n",
    "        X_num_seq, X_img_seq, y_seq = [], [], []\n",
    "        for i in range(len(X_num) - seq_length):\n",
    "            X_num_seq.append(X_num[i:i+seq_length])\n",
    "            X_img_seq.append(X_img[i+seq_length-1])  # Use image at last timestamp\n",
    "            y_seq.append(y[i+seq_length-1])\n",
    "        return np.array(X_num_seq), np.array(X_img_seq), np.array(y_seq)\n",
    "\n",
    "    X_num_seq, X_img_seq, y_seq = create_sequences(X_num, X_img, y, sequence_length)\n",
    "\n",
    "    # Train-test split\n",
    "    split_index = int(0.8 * len(X_num_seq))\n",
    "    X_num_train, X_num_test = X_num_seq[:split_index], X_num_seq[split_index:]\n",
    "    X_img_train, X_img_test = X_img_seq[:split_index], X_img_seq[split_index:]\n",
    "    y_train, y_test = y_seq[:split_index], y_seq[split_index:]\n",
    "\n",
    "    # Create model\n",
    "    num_features = X_num_train.shape[2]\n",
    "    model = create_hybrid_model(sequence_length, num_features)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        [X_img_train, X_num_train],\n",
    "        y_train,\n",
    "        validation_data=([X_img_test, X_num_test], y_test),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    return model, history, merged_data\n",
    "\n",
    "def align_data_with_images(met_data, images, image_timestamps):\n",
    "    # Create a DataFrame for image timestamps\n",
    "    image_df = pd.DataFrame({'Timestamp': image_timestamps, 'Image': images})\n",
    "    image_df['Timestamp'] = pd.to_datetime(image_df['Timestamp'])\n",
    "\n",
    "    # Merge meteorological data with images using an inner join\n",
    "    met_data['Timestamp'] = pd.to_datetime(met_data['Timestamp'])\n",
    "    merged_data = pd.merge(met_data, image_df, on='Timestamp', how='inner')\n",
    "\n",
    "    # Prepare target variables (future irradiance)\n",
    "    target = 'Irradiance'\n",
    "    for minutes in [5, 15, 30, 60]:\n",
    "        merged_data[f'Irradiance_{minutes}min_ahead'] = merged_data[target].shift(-minutes)\n",
    "\n",
    "    # Drop rows with any remaining missing values (after shifting)\n",
    "    merged_data.dropna(inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    merged_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, history, merged_data = train_hybrid_model(met_data, images, image_timestamps)\n",
    "\n",
    "# Save the model\n",
    "model.save('trainedModels/model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation/evaluate_model.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def evaluate_model(model, merged_data):\n",
    "    \"\"\"\n",
    "    Evaluates the regression model on the provided merged_data.\n",
    "    Saves evaluation plots in a uniquely named subfolder within 'evaluation_plots'.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained Keras model.\n",
    "    - merged_data: pandas DataFrame containing meteorological data and associated images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 1. Setup Directory for Saving Plots\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Define the base directory for evaluation plots\n",
    "    base_dir = 'evaluation_plots'\n",
    "    \n",
    "    # Create the base directory if it doesn't exist\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        print(f\"Created base directory for evaluation plots at '{base_dir}'.\")\n",
    "    \n",
    "    # Generate a unique subfolder name using the current timestamp\n",
    "    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_folder = os.path.join(base_dir, f'run_{run_timestamp}')\n",
    "    \n",
    "    # Create the subfolder\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    print(f\"Created run-specific directory at '{run_folder}'.\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 2. Prepare Data for Evaluation\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Define the sequence length and feature columns\n",
    "    sequence_length = 60\n",
    "    features = [\n",
    "        'Temperature', 'Humidity', 'Pressure', 'Wind Speed',\n",
    "        'Wind Dir Sin', 'Wind Dir Cos', 'Hour', 'DayOfYear'\n",
    "    ]  # Exclude direct current irradiance as a feature\n",
    "\n",
    "    # Extract feature values and target variables\n",
    "    X_num = merged_data[features].values\n",
    "    y = merged_data[[f'Irradiance_{minutes}min_ahead' for minutes in [5, 15, 30, 60]]].values\n",
    "    X_img = np.array(merged_data['Image'].tolist())\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Create Sequences\n",
    "    # -----------------------------\n",
    "    \n",
    "    def create_sequences(X_num, X_img, y, seq_length):\n",
    "        \"\"\"\n",
    "        Creates input sequences for the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - X_num: Numpy array of numerical features.\n",
    "        - X_img: Numpy array of images.\n",
    "        - y: Numpy array of target variables.\n",
    "        - seq_length: Length of the input sequences.\n",
    "        \n",
    "        Returns:\n",
    "        - Tuple of Numpy arrays: (X_num_seq, X_img_seq, y_seq)\n",
    "        \"\"\"\n",
    "        X_num_seq, X_img_seq, y_seq = [], [], []\n",
    "        for i in range(len(X_num) - seq_length):\n",
    "            X_num_seq.append(X_num[i:i+seq_length])\n",
    "            X_img_seq.append(X_img[i+seq_length-1])  # Use image at the last timestamp\n",
    "            y_seq.append(y[i+seq_length-1])\n",
    "        return np.array(X_num_seq), np.array(X_img_seq), np.array(y_seq)\n",
    "\n",
    "    # Generate sequences\n",
    "    X_num_seq, X_img_seq, y_seq = create_sequences(X_num, X_img, y, sequence_length)\n",
    "    print(f\"Created {len(X_num_seq)} sequences for evaluation.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Split Data into Test Set\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Define the split index for the last 20% as the test set\n",
    "    split_index = int(0.8 * len(X_num_seq))\n",
    "    \n",
    "    # Split the data\n",
    "    X_num_test = X_num_seq[split_index:]\n",
    "    X_img_test = X_img_seq[split_index:]\n",
    "    y_test = y_seq[split_index:]\n",
    "    \n",
    "    print(f\"Evaluation split: {len(X_num_test)} samples.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Make Predictions\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Generate predictions using the trained model\n",
    "    y_pred = model.predict([X_img_test, X_num_test])\n",
    "    print(\"Generated predictions for the test set.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6. Calculate and Save Metrics and Plots\n",
    "    # -----------------------------\n",
    "    \n",
    "    horizons = [5, 15, 30, 60]  # Prediction horizons in minutes\n",
    "    \n",
    "    # Initialize a text file to save metrics\n",
    "    metrics_file = os.path.join(run_folder, 'metrics.txt')\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        f.write(\"Evaluation Metrics:\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "    \n",
    "    for i, minutes in enumerate(horizons):\n",
    "        # Calculate metrics for each horizon\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[:, i], y_pred[:, i]))\n",
    "        mae = mean_absolute_error(y_test[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_test[:, i], y_pred[:, i])\n",
    "        metric_str = f\"{minutes}-Minute Ahead Prediction - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.2f}\"\n",
    "        print(metric_str)\n",
    "        \n",
    "        # Append metrics to the text file\n",
    "        with open(metrics_file, 'a') as f:\n",
    "            f.write(metric_str + \"\\n\")\n",
    "        \n",
    "        # Plot Actual vs Predicted\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(y_test[:, i], label='Actual', alpha=0.7)\n",
    "        plt.plot(y_pred[:, i], label='Predicted', alpha=0.7)\n",
    "        plt.title(f'{minutes}-Minute Ahead Prediction')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Irradiance (W/m²)')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_filename = f'{minutes}_min_ahead_prediction.png'\n",
    "        plot_path = os.path.join(run_folder, plot_filename)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {plot_path}\")\n",
    "        \n",
    "        # Plot Scatter of Actual vs Predicted\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(y_test[:, i], y_pred[:, i], alpha=0.5)\n",
    "        plt.plot([y_test[:, i].min(), y_test[:, i].max()],\n",
    "                 [y_test[:, i].min(), y_test[:, i].max()],\n",
    "                 'r--', lw=2)\n",
    "        plt.title(f'Actual vs Predicted Irradiance ({minutes} min Ahead)')\n",
    "        plt.xlabel('Actual Irradiance (W/m²)')\n",
    "        plt.ylabel('Predicted Irradiance (W/m²)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the scatter plot\n",
    "        scatter_filename = f'actual_vs_predicted_{minutes}_min_ahead.png'\n",
    "        scatter_path = os.path.join(run_folder, scatter_filename)\n",
    "        plt.savefig(scatter_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved scatter plot: {scatter_path}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 7. Calculate and Save Overall Metrics and Plots\n",
    "    # -----------------------------\n",
    "    \n",
    "    # Calculate overall performance metrics across all horizons\n",
    "    overall_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    overall_mae = mean_absolute_error(y_test, y_pred)\n",
    "    overall_r2 = r2_score(y_test, y_pred)\n",
    "    overall_metric_str = f\"Overall Performance - RMSE: {overall_rmse:.2f}, MAE: {overall_mae:.2f}, R²: {overall_r2:.2f}\"\n",
    "    print(overall_metric_str)\n",
    "    \n",
    "    # Append overall metrics to the text file\n",
    "    with open(metrics_file, 'a') as f:\n",
    "        f.write(\"\\n\" + overall_metric_str + \"\\n\")\n",
    "    \n",
    "    # Plot Overall Actual vs Predicted\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()],\n",
    "             [y_test.min(), y_test.max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title('Actual vs Predicted Irradiance (Overall)')\n",
    "    plt.xlabel('Actual Irradiance (W/m²)')\n",
    "    plt.ylabel('Predicted Irradiance (W/m²)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overall scatter plot\n",
    "    overall_scatter_filename = 'actual_vs_predicted_overall.png'\n",
    "    overall_scatter_path = os.path.join(run_folder, overall_scatter_filename)\n",
    "    plt.savefig(overall_scatter_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved overall scatter plot: {overall_scatter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, merged_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
